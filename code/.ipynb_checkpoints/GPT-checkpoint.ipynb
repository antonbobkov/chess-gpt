{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ae85cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess_game_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cc966b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "tokens_filepath = \"C:\\\\Users\\\\anton\\\\Documents\\\\GitHub\\\\chess-gpt\\\\data\\\\tokens.txt\"\n",
    "intermediate_filepath = \"C:\\\\Users\\\\anton\\\\Documents\\\\GitHub\\\\chess-gpt\\\\data\\\\intermediate_non_bullet.txt\"\n",
    "model_filepath = \"C:\\\\Users\\\\anton\\\\Documents\\\\GitHub\\\\chess-gpt\\\\data\\\\model_2014_non_bullet\"\n",
    "\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 64 # what is the maximum context length for predictions?\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 400\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb764c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n"
     ]
    }
   ],
   "source": [
    "with open(tokens_filepath, \"r\") as f:\n",
    "    tokens = f.read().splitlines()\n",
    "    \n",
    "tokens = [\"ZERO\", \"START\"] + tokens\n",
    "\n",
    "token_to_int = {}\n",
    "int_to_token = {}\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    token_to_int[token] = i\n",
    "    int_to_token[i] = token\n",
    "    \n",
    "def encode(lst):\n",
    "    return [token_to_int[token] for token in lst]\n",
    "def decode(lst):\n",
    "    return [int_to_token[i] for i in lst]\n",
    "\n",
    "vocab_size = len(tokens)\n",
    "print (vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1caac390",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_games = []\n",
    "with open(intermediate_filepath, \"r\") as f:\n",
    "    for line in f:\n",
    "        game = encode([\"START\"] + line.split())\n",
    "        if len(game) >= block_size + 1:\n",
    "            encoded_games.append(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde69cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33206\n",
      "143\n",
      "73\n",
      "[1, 169, 145, 81, 80, 148, 184, 208, 164, 189, 84, 212, 238, 195, 180, 213, 21, 228, 49, 63, 104, 282, 262, 23, 332, 346, 225, 231, 235, 19, 218, 59, 342, 351, 349, 332, 89, 81, 271, 291, 84, 83, 24, 295, 70, 302, 303, 298, 298, 89, 76, 69, 76, 67, 81, 128, 81, 87, 82, 87, 68, 91, 125, 152, 137, 15, 80, 8, 72, 86, 74, 85, 325, 4, 44, 7, 51, 67, 312, 16, 72, 91, 72, 328, 66, 12, 330, 322, 74, 198, 102, 25, 67, 29, 118, 130, 99, 84, 48, 86, 115, 84, 81, 89, 78, 88, 78, 107, 66, 74, 66, 33, 66, 29, 43, 33, 70, 29, 74, 33, 74, 29, 74, 33, 71, 29, 74, 70, 66, 71]\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "print (len(encoded_games))\n",
    "print (len(encoded_games[100]))\n",
    "print (len(encoded_games[200]))\n",
    "print (encoded_games[300])\n",
    "print (min([len(g) for g in encoded_games]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535f6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(encoded_games)) # first 90% will be train, rest val\n",
    "train_data = encoded_games[:n]\n",
    "val_data = encoded_games[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6cd15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_from_game(game, block_sz, rnd, offset):\n",
    "    i = rnd % (len(game) - block_sz)\n",
    "    t = game[i + offset : i + block_sz + offset]\n",
    "    if len(t) != block_sz:\n",
    "        print (game)\n",
    "        print (t)\n",
    "        print (i, offset, block_sz)\n",
    "        assert False\n",
    "    return torch.tensor(t, dtype=torch.long)\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-1, (batch_size, 2))\n",
    "    x = torch.stack([get_block_from_game(encoded_games[i[0]], block_size, i[1], 0) for i in ix])\n",
    "    y = torch.stack([get_block_from_game(encoded_games[i[0]], block_size, i[1], 1) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4025f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0ad3f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c39753e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, game):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop idx to the last block_size tokens\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        # get the predictions\n",
    "        logits, loss = model(idx_cond)\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "        legal_moves = encode(game.encoded_legal_moves())\n",
    "#         print (legal_moves)\n",
    "        if len(legal_moves) == 0:\n",
    "            return idx\n",
    "\n",
    "        for i in range(vocab_size):\n",
    "            if i not in legal_moves:\n",
    "                logits[0, i] = -100000\n",
    "        \n",
    "        # apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "        \n",
    "#         print (idx_next,  idx_next.tolist()[0], \"hi\")\n",
    "        assert idx_next.tolist()[0][0] in legal_moves\n",
    "        \n",
    "        game.make_move(decode(idx_next.tolist()[0])[0])\n",
    "        \n",
    "        # append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b578fc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.01076 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed08723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 6.0479, val loss 6.0483\n",
      "step 100: train loss 4.7590, val loss 4.7577\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10808\\3835505256.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iter in range(10000 + 1):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    if iter % (eval_interval * 10) == 0:\n",
    "        print(\"saving\")\n",
    "        torch.save(m, model_filepath + \".pt\")\n",
    "        torch.save(m.state_dict(), model_filepath + \"_state.pt\")\n",
    "        \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd112f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['START', 'Pawn:e2:TwoPush', 'Pawn:c7:TwoPush', 'N2:-1:2', 'Pawn:d7:Push', 'BL:e2', 'N1:2:-1', 'K:2:0', 'Pawn:g7:Push', 'Pawn:c2:Push', 'BD:g7', 'Pawn:d2:TwoPush', 'Pawn:c5:Push', 'Pawn:b2:Push', 'Pawn:b7:TwoPush', 'Pawn:b3:CaptureRight', 'Pawn:b5:CaptureRight', 'N1:2:1', 'Pawn:a7:TwoPush', 'R1:1:0', 'BL:a6', 'Q:a4', 'N2:-1:-2', 'BD:b2', 'K:2:0', 'Q:c2', 'N1:-1:-2', 'R2:-2:0', 'N1:1:-2', 'Q:d3', 'N2:-2:-1', 'Pawn:a2:TwoPush', 'R1:2:0', 'N2:-1:2', 'BD:f6', 'N2:-1:2', 'Q:d7', 'Pawn:g2:Push', 'Pawn:h7:TwoPush', 'BL:f3', 'Pawn:h5:Push', 'Pawn:g3:Push', 'N2:-2:1', 'Pawn:h2:Push', 'BD:g7', 'Pawn:g4:Push', 'N2:2:-1', 'N1:-2:1', 'N2:-1:-2', 'Q:d2', 'N2:1:-2', 'Q:c2', 'Pawn:f7:Push', 'N1:2:-1', 'K:1:-1', 'Q:d1', 'R2:1:0', 'N1:2:-1', 'R1:-1:0', 'Q:d2', 'R1:-1:0', 'R1:1:0', 'R2:-3:0', 'Q:e1', 'R1:1:0', 'Q:d2', 'R1:0:-6', 'N1:1:2', 'Q:a4', 'Q:e2', 'R1:0:2', 'K:-1:0', 'Q:a2', 'R1:-2:0', 'Q:b3', 'Q:d2', 'R2:1:0', 'N1:-2:-1', 'Q:b1', 'Q:c1', 'Q:a2', 'K:1:1', 'R2:-1:0', 'Q:d2', 'Q:b3', 'R1:0:2', 'Q:a4', 'K:1:0', 'R2:-2:0', 'Q:c2', 'Q:c2', 'K:0:-1', 'R2:3:0', 'N1:2:1', 'R2:-1:0', 'N1:-2:-1', 'R2:1:0', 'N1:2:1', 'R2:1:0', 'K:-1:0', 'R2:-1:0']\n",
      "1. e4 c5 2. Nf3 d6 3. Be2 Nd7 4. O-O g6 5. c3 Bg7 6. d4 c4 7. b3 b5 8. bxc4 bxc4 9. Nbd2 a5 10. Rb1 Ba6 11. Qa4 Nf6 12. Bb2 O-O 13. Qc2 Nc5 14. Rfd1 Nd3 15. Qxd3 Nd5 16. a4 Rc8 17. Ne5 Bf6 18. Nd7 Qxd7 19. g3 h5 20. Bf3 h4 21. g4 Nb6 22. h3 Bg7 23. g5 Nd5 24. Nb3 Nxc3 25. Qd2 Nxd1 26. Qc2 f6 27. Nd2 Kh7 28. Qxd1 Rg8 29. Nf1 Rb8 30. Qd2 Ra8 31. Rc1 Rgd8 32. Qe1 Rab8 33. Qd2 Rxb2 34. Ng3 Qxa4 35. Qe2 Rb4 36. Kf1 Qa2 37. Ra1 Qb3 38. Qd2 Re8 39. Ne2 Qb1+ 40. Qc1 Qa2 41. Kg2 Rd8 42. Qd2 Qb3 43. Ra3 Qa4 44. Kh2 Rdb8 45. Qc2 Qxc2 46. Kh1 Re8 47. Ng3 Rd8 48. Ne2 Re8 49. Ng3 Rf8 50. Kg1 Re8\n"
     ]
    }
   ],
   "source": [
    "game = chess_game_tracker.Game()\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "context[0][0] = 1\n",
    "print(decode(generate(m, idx = context, max_new_tokens=100, game=game)[0].tolist()))\n",
    "print (game.get_game_pgn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bd32776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m, model_filepath + \".pt\")\n",
    "torch.save(m.state_dict(), model_filepath + \"_state.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142dc488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
