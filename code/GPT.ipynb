{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae85cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess_game_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cc966b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "tokens_filepath = \"C:\\\\Users\\\\anton\\\\Documents\\\\GitHub\\\\chess-gpt\\\\data\\\\tokens.txt\"\n",
    "intermediate_filepath = \"C:\\\\Users\\\\anton\\\\Documents\\\\GitHub\\\\chess-gpt\\\\data\\\\intermediate_non_bullet.txt\"\n",
    "model_filepath = \"C:\\\\Users\\\\anton\\\\Documents\\\\GitHub\\\\chess-gpt\\\\data\\\\model_2014_non_bullet\"\n",
    "\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 64 # what is the maximum context length for predictions?\n",
    "eval_interval = 200\n",
    "learning_rate = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 100\n",
    "n_embd = 400\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb764c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n"
     ]
    }
   ],
   "source": [
    "with open(tokens_filepath, \"r\") as f:\n",
    "    tokens = f.read().splitlines()\n",
    "    \n",
    "tokens = [\"ZERO\", \"START\"] + tokens\n",
    "\n",
    "token_to_int = {}\n",
    "int_to_token = {}\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    token_to_int[token] = i\n",
    "    int_to_token[i] = token\n",
    "    \n",
    "def encode(lst):\n",
    "    return [token_to_int[token] for token in lst]\n",
    "def decode(lst):\n",
    "    return [int_to_token[i] for i in lst]\n",
    "\n",
    "vocab_size = len(tokens)\n",
    "print (vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1caac390",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_games = []\n",
    "with open(intermediate_filepath, \"r\") as f:\n",
    "    for line in f:\n",
    "        game = encode([\"START\"] + line.split())\n",
    "        if len(game) >= block_size + 1:\n",
    "            encoded_games.append(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde69cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33206\n",
      "143\n",
      "73\n",
      "[1, 169, 145, 81, 80, 148, 184, 208, 164, 189, 84, 212, 238, 195, 180, 213, 21, 228, 49, 63, 104, 282, 262, 23, 332, 346, 225, 231, 235, 19, 218, 59, 342, 351, 349, 332, 89, 81, 271, 291, 84, 83, 24, 295, 70, 302, 303, 298, 298, 89, 76, 69, 76, 67, 81, 128, 81, 87, 82, 87, 68, 91, 125, 152, 137, 15, 80, 8, 72, 86, 74, 85, 325, 4, 44, 7, 51, 67, 312, 16, 72, 91, 72, 328, 66, 12, 330, 322, 74, 198, 102, 25, 67, 29, 118, 130, 99, 84, 48, 86, 115, 84, 81, 89, 78, 88, 78, 107, 66, 74, 66, 33, 66, 29, 43, 33, 70, 29, 74, 33, 74, 29, 74, 33, 71, 29, 74, 70, 66, 71]\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "print (len(encoded_games))\n",
    "print (len(encoded_games[100]))\n",
    "print (len(encoded_games[200]))\n",
    "print (encoded_games[300])\n",
    "print (min([len(g) for g in encoded_games]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535f6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(encoded_games)) # first 90% will be train, rest val\n",
    "train_data = encoded_games[:n]\n",
    "val_data = encoded_games[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6cd15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_from_game(game, block_sz, rnd, offset):\n",
    "    i = rnd % (len(game) - block_sz)\n",
    "    t = game[i + offset : i + block_sz + offset]\n",
    "    if len(t) != block_sz:\n",
    "        print (game)\n",
    "        print (t)\n",
    "        print (i, offset, block_sz)\n",
    "        assert False\n",
    "    return torch.tensor(t, dtype=torch.long)\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-1, (batch_size, 2))\n",
    "    x = torch.stack([get_block_from_game(encoded_games[i[0]], block_size, i[1], 0) for i in ix])\n",
    "    y = torch.stack([get_block_from_game(encoded_games[i[0]], block_size, i[1], 1) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4025f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0ad3f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58cce38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, game):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop idx to the last block_size tokens\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        # get the predictions\n",
    "        logits, loss = model(idx_cond)\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "        legal_moves = encode(game.encoded_legal_moves())\n",
    "#         print (legal_moves)\n",
    "        if len(legal_moves) == 0:\n",
    "            return idx\n",
    "\n",
    "        for i in range(vocab_size):\n",
    "            if i not in legal_moves:\n",
    "                logits[0, i] = -100000\n",
    "        \n",
    "        # apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "        \n",
    "#         print (idx_next,  idx_next.tolist()[0], \"hi\")\n",
    "        assert idx_next.tolist()[0][0] in legal_moves\n",
    "        \n",
    "        game.make_move(decode(idx_next.tolist()[0])[0])\n",
    "        \n",
    "        # append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b578fc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.01076 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "353901f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eed08723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.1983, val loss 2.1795\n",
      "saving\n",
      "step 200: train loss 2.0467, val loss 2.0081\n",
      "step 400: train loss 1.9899, val loss 1.9546\n",
      "step 600: train loss 1.9503, val loss 1.8977\n",
      "step 800: train loss 1.9229, val loss 1.9051\n",
      "step 1000: train loss 1.8984, val loss 1.8627\n",
      "step 1200: train loss 1.8568, val loss 1.8577\n",
      "step 1400: train loss 1.8626, val loss 1.8331\n",
      "step 1600: train loss 1.8381, val loss 1.8275\n",
      "step 1800: train loss 1.8398, val loss 1.7935\n",
      "step 2000: train loss 1.8423, val loss 1.7659\n",
      "saving\n",
      "step 2200: train loss 1.8122, val loss 1.7684\n",
      "step 2400: train loss 1.8001, val loss 1.7837\n",
      "step 2600: train loss 1.7892, val loss 1.7454\n",
      "step 2800: train loss 1.7684, val loss 1.7429\n",
      "step 3000: train loss 1.7561, val loss 1.7383\n",
      "step 3200: train loss 1.7645, val loss 1.7326\n",
      "step 3400: train loss 1.7451, val loss 1.7067\n",
      "step 3600: train loss 1.7583, val loss 1.6972\n",
      "step 3800: train loss 1.7336, val loss 1.7075\n",
      "step 4000: train loss 1.7300, val loss 1.6875\n",
      "saving\n",
      "step 4200: train loss 1.7032, val loss 1.6613\n",
      "step 4400: train loss 1.7205, val loss 1.6792\n",
      "step 4600: train loss 1.6866, val loss 1.6665\n",
      "step 4800: train loss 1.6852, val loss 1.6344\n",
      "step 5000: train loss 1.7042, val loss 1.6588\n",
      "step 5200: train loss 1.6825, val loss 1.6550\n",
      "step 5400: train loss 1.6877, val loss 1.6424\n",
      "step 5600: train loss 1.6681, val loss 1.6402\n",
      "step 5800: train loss 1.6630, val loss 1.6107\n",
      "step 6000: train loss 1.6479, val loss 1.6427\n",
      "saving\n",
      "step 6200: train loss 1.6503, val loss 1.6218\n",
      "step 6400: train loss 1.6312, val loss 1.5959\n",
      "step 6600: train loss 1.6439, val loss 1.6066\n",
      "step 6800: train loss 1.6195, val loss 1.5753\n",
      "step 7000: train loss 1.6291, val loss 1.5759\n",
      "step 7200: train loss 1.6225, val loss 1.5901\n",
      "step 7400: train loss 1.6159, val loss 1.5809\n",
      "step 7600: train loss 1.6306, val loss 1.5748\n",
      "step 7800: train loss 1.6089, val loss 1.5700\n",
      "step 8000: train loss 1.5843, val loss 1.5537\n",
      "saving\n",
      "step 8200: train loss 1.6023, val loss 1.5452\n",
      "step 8400: train loss 1.6000, val loss 1.5108\n",
      "step 8600: train loss 1.5967, val loss 1.5538\n",
      "step 8800: train loss 1.5641, val loss 1.5280\n",
      "step 9000: train loss 1.5840, val loss 1.5042\n",
      "step 9200: train loss 1.5668, val loss 1.5109\n",
      "step 9400: train loss 1.5631, val loss 1.5231\n",
      "step 9600: train loss 1.5695, val loss 1.5246\n",
      "step 9800: train loss 1.5539, val loss 1.5181\n",
      "step 10000: train loss 1.5554, val loss 1.5097\n",
      "saving\n",
      "step 10200: train loss 1.5423, val loss 1.4927\n",
      "step 10400: train loss 1.5514, val loss 1.5001\n",
      "step 10600: train loss 1.5198, val loss 1.5056\n",
      "step 10800: train loss 1.5192, val loss 1.4887\n",
      "step 11000: train loss 1.5307, val loss 1.5191\n",
      "step 11200: train loss 1.5320, val loss 1.4752\n",
      "step 11400: train loss 1.5149, val loss 1.4909\n",
      "step 11600: train loss 1.5338, val loss 1.4698\n",
      "step 11800: train loss 1.5284, val loss 1.4925\n",
      "step 12000: train loss 1.5081, val loss 1.4750\n",
      "saving\n",
      "step 12200: train loss 1.4740, val loss 1.4745\n",
      "step 12400: train loss 1.5077, val loss 1.4563\n",
      "step 12600: train loss 1.5064, val loss 1.4576\n",
      "step 12800: train loss 1.5034, val loss 1.4528\n",
      "step 13000: train loss 1.4865, val loss 1.4512\n",
      "step 13200: train loss 1.4955, val loss 1.4612\n",
      "step 13400: train loss 1.4908, val loss 1.4370\n",
      "step 13600: train loss 1.4920, val loss 1.4461\n",
      "step 13800: train loss 1.4479, val loss 1.4576\n",
      "step 14000: train loss 1.4872, val loss 1.4145\n",
      "saving\n",
      "step 14200: train loss 1.4870, val loss 1.4235\n",
      "step 14400: train loss 1.4594, val loss 1.4370\n",
      "step 14600: train loss 1.4734, val loss 1.4148\n",
      "step 14800: train loss 1.4731, val loss 1.4028\n",
      "step 15000: train loss 1.4413, val loss 1.4129\n",
      "step 15200: train loss 1.4491, val loss 1.4257\n",
      "step 15400: train loss 1.4400, val loss 1.4075\n",
      "step 15600: train loss 1.4348, val loss 1.3904\n",
      "step 15800: train loss 1.4244, val loss 1.3816\n",
      "step 16000: train loss 1.4308, val loss 1.3699\n",
      "saving\n",
      "step 16200: train loss 1.4133, val loss 1.4015\n",
      "step 16400: train loss 1.4173, val loss 1.3816\n",
      "step 16600: train loss 1.4116, val loss 1.3623\n",
      "step 16800: train loss 1.3993, val loss 1.3884\n",
      "step 17000: train loss 1.4309, val loss 1.3683\n",
      "step 17200: train loss 1.3949, val loss 1.3748\n",
      "step 17400: train loss 1.4099, val loss 1.3705\n",
      "step 17600: train loss 1.4091, val loss 1.3574\n",
      "step 17800: train loss 1.3872, val loss 1.3541\n",
      "step 18000: train loss 1.3896, val loss 1.3620\n",
      "saving\n",
      "step 18200: train loss 1.4008, val loss 1.3493\n",
      "step 18400: train loss 1.4013, val loss 1.3402\n",
      "step 18600: train loss 1.3827, val loss 1.3488\n",
      "step 18800: train loss 1.3986, val loss 1.3494\n",
      "step 19000: train loss 1.3813, val loss 1.3549\n",
      "step 19200: train loss 1.3943, val loss 1.3371\n",
      "step 19400: train loss 1.3621, val loss 1.3369\n",
      "step 19600: train loss 1.3725, val loss 1.3504\n",
      "step 19800: train loss 1.3697, val loss 1.3253\n",
      "step 20000: train loss 1.3825, val loss 1.3217\n",
      "saving\n",
      "step 20200: train loss 1.3506, val loss 1.3276\n",
      "step 20400: train loss 1.3484, val loss 1.3192\n",
      "step 20600: train loss 1.3428, val loss 1.3103\n",
      "step 20800: train loss 1.3463, val loss 1.3084\n",
      "step 21000: train loss 1.3484, val loss 1.3142\n",
      "step 21200: train loss 1.3324, val loss 1.2992\n",
      "step 21400: train loss 1.3515, val loss 1.2948\n",
      "step 21600: train loss 1.3421, val loss 1.2939\n",
      "step 21800: train loss 1.3300, val loss 1.2870\n",
      "step 22000: train loss 1.3289, val loss 1.2885\n",
      "saving\n",
      "step 22200: train loss 1.3280, val loss 1.2804\n",
      "step 22400: train loss 1.3359, val loss 1.2994\n",
      "step 22600: train loss 1.3129, val loss 1.2584\n",
      "step 22800: train loss 1.3177, val loss 1.3023\n",
      "step 23000: train loss 1.3324, val loss 1.2843\n",
      "step 23200: train loss 1.3181, val loss 1.2647\n",
      "step 23400: train loss 1.3177, val loss 1.2719\n",
      "step 23600: train loss 1.3193, val loss 1.2957\n",
      "step 23800: train loss 1.3088, val loss 1.2748\n",
      "step 24000: train loss 1.3224, val loss 1.2858\n",
      "saving\n",
      "step 24200: train loss 1.3069, val loss 1.2537\n",
      "step 24400: train loss 1.2871, val loss 1.2473\n",
      "step 24600: train loss 1.3126, val loss 1.2695\n",
      "step 24800: train loss 1.2824, val loss 1.2477\n",
      "step 25000: train loss 1.2818, val loss 1.2382\n",
      "step 25200: train loss 1.2932, val loss 1.2663\n",
      "step 25400: train loss 1.2870, val loss 1.2487\n",
      "step 25600: train loss 1.2840, val loss 1.2297\n",
      "step 25800: train loss 1.2820, val loss 1.2610\n",
      "step 26000: train loss 1.2704, val loss 1.2434\n",
      "saving\n",
      "step 26200: train loss 1.2806, val loss 1.2350\n",
      "step 26400: train loss 1.2802, val loss 1.2392\n",
      "step 26600: train loss 1.2559, val loss 1.2421\n",
      "step 26800: train loss 1.2713, val loss 1.2410\n",
      "step 27000: train loss 1.2428, val loss 1.2262\n",
      "step 27200: train loss 1.2660, val loss 1.2278\n",
      "step 27400: train loss 1.2483, val loss 1.2285\n",
      "step 27600: train loss 1.2699, val loss 1.2070\n",
      "step 27800: train loss 1.2649, val loss 1.2351\n",
      "step 28000: train loss 1.2513, val loss 1.2182\n",
      "saving\n",
      "step 28200: train loss 1.2708, val loss 1.1988\n",
      "step 28400: train loss 1.2682, val loss 1.2064\n",
      "step 28600: train loss 1.2563, val loss 1.1983\n",
      "step 28800: train loss 1.2528, val loss 1.2024\n",
      "step 29000: train loss 1.2406, val loss 1.2000\n",
      "step 29200: train loss 1.2388, val loss 1.1932\n",
      "step 29400: train loss 1.2419, val loss 1.2187\n",
      "step 29600: train loss 1.2388, val loss 1.1730\n",
      "step 29800: train loss 1.2322, val loss 1.1893\n",
      "step 30000: train loss 1.2594, val loss 1.1923\n",
      "saving\n",
      "step 30200: train loss 1.2305, val loss 1.1897\n",
      "step 30400: train loss 1.2343, val loss 1.1958\n",
      "step 30600: train loss 1.2368, val loss 1.1959\n",
      "step 30800: train loss 1.2167, val loss 1.1548\n",
      "step 31000: train loss 1.1993, val loss 1.1768\n",
      "step 31200: train loss 1.2239, val loss 1.1915\n",
      "step 31400: train loss 1.2102, val loss 1.1595\n",
      "step 31600: train loss 1.2156, val loss 1.1679\n",
      "step 31800: train loss 1.1993, val loss 1.1645\n",
      "step 32000: train loss 1.2189, val loss 1.1371\n",
      "saving\n",
      "step 32200: train loss 1.2080, val loss 1.1750\n",
      "step 32400: train loss 1.1974, val loss 1.1830\n",
      "step 32600: train loss 1.1876, val loss 1.1571\n",
      "step 32800: train loss 1.2055, val loss 1.1557\n",
      "step 33000: train loss 1.1999, val loss 1.1750\n",
      "step 33200: train loss 1.1765, val loss 1.1614\n",
      "step 33400: train loss 1.1994, val loss 1.1672\n",
      "step 33600: train loss 1.2095, val loss 1.1482\n",
      "step 33800: train loss 1.1919, val loss 1.1267\n",
      "step 34000: train loss 1.2041, val loss 1.1413\n",
      "saving\n",
      "step 34200: train loss 1.1813, val loss 1.1358\n",
      "step 34400: train loss 1.1853, val loss 1.1398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 34600: train loss 1.1886, val loss 1.1431\n",
      "step 34800: train loss 1.1759, val loss 1.1462\n",
      "step 35000: train loss 1.1690, val loss 1.1457\n",
      "step 35200: train loss 1.1731, val loss 1.1497\n",
      "step 35400: train loss 1.1941, val loss 1.1458\n",
      "step 35600: train loss 1.1861, val loss 1.1398\n",
      "step 35800: train loss 1.1629, val loss 1.1298\n",
      "step 36000: train loss 1.1806, val loss 1.1254\n",
      "saving\n",
      "step 36200: train loss 1.1648, val loss 1.1409\n",
      "step 36400: train loss 1.1670, val loss 1.1213\n",
      "step 36600: train loss 1.1617, val loss 1.1400\n",
      "step 36800: train loss 1.1775, val loss 1.1389\n",
      "step 37000: train loss 1.1517, val loss 1.1131\n",
      "step 37200: train loss 1.1526, val loss 1.1039\n",
      "step 37400: train loss 1.1581, val loss 1.1117\n",
      "step 37600: train loss 1.1640, val loss 1.1213\n",
      "step 37800: train loss 1.1524, val loss 1.0948\n",
      "step 38000: train loss 1.1611, val loss 1.1173\n",
      "saving\n",
      "step 38200: train loss 1.1620, val loss 1.1132\n",
      "step 38400: train loss 1.1526, val loss 1.1023\n",
      "step 38600: train loss 1.1493, val loss 1.1058\n",
      "step 38800: train loss 1.1413, val loss 1.1010\n",
      "step 39000: train loss 1.1413, val loss 1.0880\n",
      "step 39200: train loss 1.1556, val loss 1.0970\n",
      "step 39400: train loss 1.1633, val loss 1.1138\n",
      "step 39600: train loss 1.1294, val loss 1.0973\n",
      "step 39800: train loss 1.1336, val loss 1.0909\n",
      "step 40000: train loss 1.1377, val loss 1.1021\n",
      "saving\n",
      "step 40200: train loss 1.1280, val loss 1.0869\n",
      "step 40400: train loss 1.1475, val loss 1.1009\n",
      "step 40600: train loss 1.1235, val loss 1.0972\n",
      "step 40800: train loss 1.1285, val loss 1.0856\n",
      "step 41000: train loss 1.1435, val loss 1.0843\n",
      "step 41200: train loss 1.1209, val loss 1.0905\n",
      "step 41400: train loss 1.1165, val loss 1.0821\n",
      "step 41600: train loss 1.1313, val loss 1.0844\n",
      "step 41800: train loss 1.1035, val loss 1.0919\n",
      "step 42000: train loss 1.1208, val loss 1.0969\n",
      "saving\n",
      "step 42200: train loss 1.1158, val loss 1.0811\n",
      "step 42400: train loss 1.1123, val loss 1.0847\n",
      "step 42600: train loss 1.1039, val loss 1.0713\n",
      "step 42800: train loss 1.0996, val loss 1.0790\n",
      "step 43000: train loss 1.1075, val loss 1.0789\n",
      "step 43200: train loss 1.1030, val loss 1.0816\n",
      "step 43400: train loss 1.1155, val loss 1.0731\n",
      "step 43600: train loss 1.1075, val loss 1.0684\n",
      "step 43800: train loss 1.1057, val loss 1.0802\n",
      "step 44000: train loss 1.0979, val loss 1.0892\n",
      "saving\n",
      "step 44200: train loss 1.0926, val loss 1.0667\n",
      "step 44400: train loss 1.0964, val loss 1.0587\n",
      "step 44600: train loss 1.0908, val loss 1.0685\n",
      "step 44800: train loss 1.0928, val loss 1.0699\n",
      "step 45000: train loss 1.0876, val loss 1.0541\n",
      "step 45200: train loss 1.1057, val loss 1.0540\n",
      "step 45400: train loss 1.0944, val loss 1.0525\n",
      "step 45600: train loss 1.0921, val loss 1.0538\n",
      "step 45800: train loss 1.0996, val loss 1.0615\n",
      "step 46000: train loss 1.0763, val loss 1.0579\n",
      "saving\n",
      "step 46200: train loss 1.1013, val loss 1.0506\n",
      "step 46400: train loss 1.0813, val loss 1.0574\n",
      "step 46600: train loss 1.0915, val loss 1.0603\n",
      "step 46800: train loss 1.0692, val loss 1.0451\n",
      "step 47000: train loss 1.0776, val loss 1.0474\n",
      "step 47200: train loss 1.0902, val loss 1.0426\n",
      "step 47400: train loss 1.0860, val loss 1.0474\n",
      "step 47600: train loss 1.0808, val loss 1.0436\n",
      "step 47800: train loss 1.0954, val loss 1.0343\n",
      "step 48000: train loss 1.0666, val loss 1.0410\n",
      "saving\n",
      "step 48200: train loss 1.0761, val loss 1.0429\n",
      "step 48400: train loss 1.0818, val loss 1.0341\n",
      "step 48600: train loss 1.0606, val loss 1.0304\n",
      "step 48800: train loss 1.0753, val loss 1.0197\n",
      "step 49000: train loss 1.0872, val loss 1.0348\n",
      "step 49200: train loss 1.0813, val loss 1.0385\n",
      "step 49400: train loss 1.0678, val loss 1.0188\n",
      "step 49600: train loss 1.0770, val loss 1.0254\n",
      "step 49800: train loss 1.0727, val loss 1.0107\n",
      "step 50000: train loss 1.0726, val loss 1.0368\n",
      "saving\n",
      "step 50200: train loss 1.0708, val loss 1.0475\n",
      "step 50400: train loss 1.0550, val loss 1.0206\n",
      "step 50600: train loss 1.0634, val loss 1.0309\n",
      "step 50800: train loss 1.0800, val loss 1.0062\n",
      "step 51000: train loss 1.0658, val loss 0.9976\n",
      "step 51200: train loss 1.0474, val loss 1.0214\n",
      "step 51400: train loss 1.0364, val loss 1.0356\n",
      "step 51600: train loss 1.0549, val loss 1.0144\n",
      "step 51800: train loss 1.0829, val loss 1.0180\n",
      "step 52000: train loss 1.0516, val loss 1.0189\n",
      "saving\n",
      "step 52200: train loss 1.0418, val loss 1.0044\n",
      "step 52400: train loss 1.0565, val loss 0.9958\n",
      "step 52600: train loss 1.0469, val loss 1.0130\n",
      "step 52800: train loss 1.0625, val loss 1.0001\n",
      "step 53000: train loss 1.0566, val loss 1.0020\n",
      "step 53200: train loss 1.0559, val loss 0.9997\n",
      "step 53400: train loss 1.0415, val loss 0.9926\n",
      "step 53600: train loss 1.0450, val loss 1.0058\n",
      "step 53800: train loss 1.0456, val loss 1.0084\n",
      "step 54000: train loss 1.0380, val loss 0.9991\n",
      "saving\n",
      "step 54200: train loss 1.0240, val loss 0.9987\n",
      "step 54400: train loss 1.0473, val loss 0.9979\n",
      "step 54600: train loss 1.0365, val loss 1.0018\n",
      "step 54800: train loss 1.0336, val loss 0.9958\n",
      "step 55000: train loss 1.0336, val loss 1.0236\n",
      "step 55200: train loss 1.0334, val loss 1.0045\n",
      "step 55400: train loss 1.0386, val loss 1.0025\n",
      "step 55600: train loss 1.0260, val loss 0.9791\n",
      "step 55800: train loss 1.0286, val loss 0.9790\n",
      "step 56000: train loss 1.0215, val loss 0.9709\n",
      "saving\n",
      "step 56200: train loss 1.0216, val loss 0.9915\n",
      "step 56400: train loss 1.0203, val loss 0.9937\n",
      "step 56600: train loss 1.0099, val loss 0.9956\n",
      "step 56800: train loss 1.0071, val loss 0.9853\n",
      "step 57000: train loss 1.0095, val loss 0.9754\n",
      "step 57200: train loss 1.0200, val loss 0.9875\n",
      "step 57400: train loss 1.0298, val loss 0.9741\n",
      "step 57600: train loss 1.0126, val loss 0.9851\n",
      "step 57800: train loss 1.0149, val loss 0.9690\n",
      "step 58000: train loss 0.9873, val loss 0.9999\n",
      "saving\n",
      "step 58200: train loss 1.0220, val loss 0.9641\n",
      "step 58400: train loss 1.0231, val loss 0.9831\n",
      "step 58600: train loss 1.0339, val loss 0.9947\n",
      "step 58800: train loss 1.0065, val loss 0.9802\n",
      "step 59000: train loss 1.0060, val loss 0.9711\n",
      "step 59200: train loss 1.0180, val loss 0.9706\n",
      "step 59400: train loss 1.0079, val loss 0.9692\n",
      "step 59600: train loss 0.9999, val loss 0.9724\n",
      "step 59800: train loss 1.0174, val loss 0.9681\n",
      "step 60000: train loss 1.0099, val loss 0.9680\n",
      "saving\n",
      "step 60200: train loss 0.9937, val loss 0.9733\n",
      "step 60400: train loss 1.0155, val loss 0.9657\n",
      "step 60600: train loss 1.0141, val loss 0.9751\n",
      "step 60800: train loss 0.9841, val loss 0.9522\n",
      "step 61000: train loss 1.0173, val loss 0.9652\n",
      "step 61200: train loss 0.9929, val loss 0.9669\n",
      "step 61400: train loss 0.9939, val loss 0.9478\n",
      "step 61600: train loss 0.9999, val loss 0.9570\n",
      "step 61800: train loss 1.0000, val loss 0.9708\n",
      "step 62000: train loss 0.9899, val loss 0.9539\n",
      "saving\n",
      "step 62200: train loss 1.0033, val loss 0.9516\n",
      "step 62400: train loss 0.9868, val loss 0.9553\n",
      "step 62600: train loss 1.0082, val loss 0.9540\n",
      "step 62800: train loss 0.9876, val loss 0.9657\n",
      "step 63000: train loss 0.9979, val loss 0.9597\n",
      "step 63200: train loss 1.0152, val loss 0.9409\n",
      "step 63400: train loss 0.9932, val loss 0.9485\n",
      "step 63600: train loss 0.9878, val loss 0.9521\n",
      "step 63800: train loss 0.9952, val loss 0.9511\n",
      "step 64000: train loss 1.0017, val loss 0.9506\n",
      "saving\n",
      "step 64200: train loss 0.9859, val loss 0.9334\n",
      "step 64400: train loss 0.9762, val loss 0.9584\n",
      "step 64600: train loss 0.9998, val loss 0.9387\n",
      "step 64800: train loss 0.9832, val loss 0.9405\n",
      "step 65000: train loss 0.9732, val loss 0.9338\n",
      "step 65200: train loss 0.9895, val loss 0.9618\n",
      "step 65400: train loss 0.9783, val loss 0.9188\n",
      "step 65600: train loss 0.9859, val loss 0.9507\n",
      "step 65800: train loss 0.9742, val loss 0.9499\n",
      "step 66000: train loss 0.9663, val loss 0.9281\n",
      "saving\n",
      "step 66200: train loss 0.9880, val loss 0.9377\n",
      "step 66400: train loss 0.9801, val loss 0.9350\n",
      "step 66600: train loss 0.9802, val loss 0.9197\n",
      "step 66800: train loss 0.9638, val loss 0.9352\n",
      "step 67000: train loss 0.9612, val loss 0.9244\n",
      "step 67200: train loss 0.9452, val loss 0.9279\n",
      "step 67400: train loss 0.9554, val loss 0.9173\n",
      "step 67600: train loss 0.9613, val loss 0.9418\n",
      "step 67800: train loss 0.9706, val loss 0.9295\n",
      "step 68000: train loss 0.9713, val loss 0.9252\n",
      "saving\n",
      "step 68200: train loss 0.9697, val loss 0.9335\n",
      "step 68400: train loss 0.9743, val loss 0.9543\n",
      "step 68600: train loss 0.9477, val loss 0.9226\n",
      "step 68800: train loss 0.9680, val loss 0.9383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 69000: train loss 0.9601, val loss 0.9276\n",
      "step 69200: train loss 0.9630, val loss 0.9217\n",
      "step 69400: train loss 0.9640, val loss 0.9466\n",
      "step 69600: train loss 0.9573, val loss 0.9066\n",
      "step 69800: train loss 0.9430, val loss 0.9080\n",
      "step 70000: train loss 0.9553, val loss 0.9117\n",
      "saving\n",
      "step 70200: train loss 0.9524, val loss 0.9184\n",
      "step 70400: train loss 0.9599, val loss 0.9150\n",
      "step 70600: train loss 0.9605, val loss 0.9177\n",
      "step 70800: train loss 0.9573, val loss 0.9444\n",
      "step 71000: train loss 0.9591, val loss 0.9438\n",
      "step 71200: train loss 0.9342, val loss 0.9104\n",
      "step 71400: train loss 0.9620, val loss 0.9175\n",
      "step 71600: train loss 0.9475, val loss 0.9098\n",
      "step 71800: train loss 0.9433, val loss 0.9095\n",
      "step 72000: train loss 0.9584, val loss 0.8942\n",
      "saving\n",
      "step 72200: train loss 0.9413, val loss 0.9097\n",
      "step 72400: train loss 0.9598, val loss 0.9141\n",
      "step 72600: train loss 0.9403, val loss 0.9240\n",
      "step 72800: train loss 0.9366, val loss 0.9239\n",
      "step 73000: train loss 0.9444, val loss 0.9078\n",
      "step 73200: train loss 0.9490, val loss 0.9021\n",
      "step 73400: train loss 0.9612, val loss 0.9160\n",
      "step 73600: train loss 0.9346, val loss 0.9001\n",
      "step 73800: train loss 0.9429, val loss 0.8980\n",
      "step 74000: train loss 0.9452, val loss 0.9180\n",
      "saving\n",
      "step 74200: train loss 0.9494, val loss 0.9191\n",
      "step 74400: train loss 0.9455, val loss 0.9026\n",
      "step 74600: train loss 0.9518, val loss 0.8900\n",
      "step 74800: train loss 0.9428, val loss 0.9145\n",
      "step 75000: train loss 0.9420, val loss 0.8984\n",
      "step 75200: train loss 0.9280, val loss 0.9067\n",
      "step 75400: train loss 0.9299, val loss 0.8842\n",
      "step 75600: train loss 0.9427, val loss 0.9228\n",
      "step 75800: train loss 0.9378, val loss 0.8969\n",
      "step 76000: train loss 0.9394, val loss 0.9178\n",
      "saving\n",
      "step 76200: train loss 0.9251, val loss 0.9186\n",
      "step 76400: train loss 0.9322, val loss 0.9073\n",
      "step 76600: train loss 0.9367, val loss 0.9051\n",
      "step 76800: train loss 0.9174, val loss 0.9005\n",
      "step 77000: train loss 0.9495, val loss 0.9085\n",
      "step 77200: train loss 0.9201, val loss 0.9155\n",
      "step 77400: train loss 0.9378, val loss 0.9054\n",
      "step 77600: train loss 0.9379, val loss 0.8959\n",
      "step 77800: train loss 0.9299, val loss 0.8921\n",
      "step 78000: train loss 0.9473, val loss 0.8949\n",
      "saving\n",
      "step 78200: train loss 0.9156, val loss 0.8912\n",
      "step 78400: train loss 0.9363, val loss 0.8909\n",
      "step 78600: train loss 0.9316, val loss 0.8835\n",
      "step 78800: train loss 0.9284, val loss 0.8960\n",
      "step 79000: train loss 0.9168, val loss 0.8839\n",
      "step 79200: train loss 0.9094, val loss 0.8808\n",
      "step 79400: train loss 0.9140, val loss 0.8769\n",
      "step 79600: train loss 0.8999, val loss 0.9030\n",
      "step 79800: train loss 0.9233, val loss 0.8990\n",
      "step 80000: train loss 0.9284, val loss 0.8877\n",
      "saving\n",
      "step 80200: train loss 0.9343, val loss 0.8831\n",
      "step 80400: train loss 0.9171, val loss 0.8844\n",
      "step 80600: train loss 0.9159, val loss 0.8974\n",
      "step 80800: train loss 0.9224, val loss 0.8788\n",
      "step 81000: train loss 0.9292, val loss 0.8920\n",
      "step 81200: train loss 0.9141, val loss 0.8773\n",
      "step 81400: train loss 0.9153, val loss 0.8690\n",
      "step 81600: train loss 0.9085, val loss 0.8800\n",
      "step 81800: train loss 0.9194, val loss 0.8872\n",
      "step 82000: train loss 0.9126, val loss 0.8660\n",
      "saving\n",
      "step 82200: train loss 0.9240, val loss 0.8883\n",
      "step 82400: train loss 0.8966, val loss 0.8773\n",
      "step 82600: train loss 0.9081, val loss 0.8677\n",
      "step 82800: train loss 0.9365, val loss 0.8746\n",
      "step 83000: train loss 0.9109, val loss 0.8853\n",
      "step 83200: train loss 0.8860, val loss 0.8769\n",
      "step 83400: train loss 0.9262, val loss 0.8762\n",
      "step 83600: train loss 0.9067, val loss 0.8619\n",
      "step 83800: train loss 0.9031, val loss 0.8665\n",
      "step 84000: train loss 0.8889, val loss 0.8782\n",
      "saving\n",
      "step 84200: train loss 0.9115, val loss 0.8647\n",
      "step 84400: train loss 0.9028, val loss 0.8763\n",
      "step 84600: train loss 0.9025, val loss 0.8801\n",
      "step 84800: train loss 0.9102, val loss 0.8684\n",
      "step 85000: train loss 0.8756, val loss 0.8587\n",
      "step 85200: train loss 0.9093, val loss 0.8522\n",
      "step 85400: train loss 0.9247, val loss 0.8489\n",
      "step 85600: train loss 0.8857, val loss 0.8707\n",
      "step 85800: train loss 0.8888, val loss 0.8671\n",
      "step 86000: train loss 0.9051, val loss 0.8642\n",
      "saving\n",
      "step 86200: train loss 0.8905, val loss 0.8559\n",
      "step 86400: train loss 0.8955, val loss 0.8560\n",
      "step 86600: train loss 0.9026, val loss 0.8705\n",
      "step 86800: train loss 0.8955, val loss 0.8521\n",
      "step 87000: train loss 0.8895, val loss 0.8528\n",
      "step 87200: train loss 0.9051, val loss 0.8652\n",
      "step 87400: train loss 0.9036, val loss 0.8509\n",
      "step 87600: train loss 0.8910, val loss 0.8449\n",
      "step 87800: train loss 0.9029, val loss 0.8651\n",
      "step 88000: train loss 0.8918, val loss 0.8506\n",
      "saving\n",
      "step 88200: train loss 0.8949, val loss 0.8608\n",
      "step 88400: train loss 0.8740, val loss 0.8564\n",
      "step 88600: train loss 0.8816, val loss 0.8619\n",
      "step 88800: train loss 0.8955, val loss 0.8643\n",
      "step 89000: train loss 0.9057, val loss 0.8617\n",
      "step 89200: train loss 0.8899, val loss 0.8477\n",
      "step 89400: train loss 0.8815, val loss 0.8492\n",
      "step 89600: train loss 0.8792, val loss 0.8543\n",
      "step 89800: train loss 0.8888, val loss 0.8453\n",
      "step 90000: train loss 0.8814, val loss 0.8587\n",
      "saving\n",
      "step 90200: train loss 0.8872, val loss 0.8387\n",
      "step 90400: train loss 0.8972, val loss 0.8489\n",
      "step 90600: train loss 0.8951, val loss 0.8426\n",
      "step 90800: train loss 0.8903, val loss 0.8510\n",
      "step 91000: train loss 0.8875, val loss 0.8735\n",
      "step 91200: train loss 0.8912, val loss 0.8393\n",
      "step 91400: train loss 0.8788, val loss 0.8501\n",
      "step 91600: train loss 0.8852, val loss 0.8482\n",
      "step 91800: train loss 0.8608, val loss 0.8550\n",
      "step 92000: train loss 0.8787, val loss 0.8446\n",
      "saving\n",
      "step 92200: train loss 0.8760, val loss 0.8614\n",
      "step 92400: train loss 0.8972, val loss 0.8378\n",
      "step 92600: train loss 0.8809, val loss 0.8385\n",
      "step 92800: train loss 0.8792, val loss 0.8427\n",
      "step 93000: train loss 0.8798, val loss 0.8387\n",
      "step 93200: train loss 0.8890, val loss 0.8330\n",
      "step 93400: train loss 0.8775, val loss 0.8430\n",
      "step 93600: train loss 0.8766, val loss 0.8400\n",
      "step 93800: train loss 0.8685, val loss 0.8308\n",
      "step 94000: train loss 0.8744, val loss 0.8409\n",
      "saving\n",
      "step 94200: train loss 0.8672, val loss 0.8305\n",
      "step 94400: train loss 0.8693, val loss 0.8578\n",
      "step 94600: train loss 0.8683, val loss 0.8312\n",
      "step 94800: train loss 0.8639, val loss 0.8315\n",
      "step 95000: train loss 0.8484, val loss 0.8477\n",
      "step 95200: train loss 0.8821, val loss 0.8420\n",
      "step 95400: train loss 0.8847, val loss 0.8409\n",
      "step 95600: train loss 0.8731, val loss 0.8407\n",
      "step 95800: train loss 0.8882, val loss 0.8361\n",
      "step 96000: train loss 0.8694, val loss 0.8373\n",
      "saving\n",
      "step 96200: train loss 0.8797, val loss 0.8303\n",
      "step 96400: train loss 0.8593, val loss 0.8371\n",
      "step 96600: train loss 0.8758, val loss 0.8345\n",
      "step 96800: train loss 0.8591, val loss 0.8330\n",
      "step 97000: train loss 0.8575, val loss 0.8263\n",
      "step 97200: train loss 0.8725, val loss 0.8388\n",
      "step 97400: train loss 0.8812, val loss 0.8383\n",
      "step 97600: train loss 0.8606, val loss 0.8377\n",
      "step 97800: train loss 0.8615, val loss 0.8302\n",
      "step 98000: train loss 0.8847, val loss 0.8301\n",
      "saving\n",
      "step 98200: train loss 0.8688, val loss 0.8209\n",
      "step 98400: train loss 0.8752, val loss 0.8312\n",
      "step 98600: train loss 0.8476, val loss 0.8220\n",
      "step 98800: train loss 0.8550, val loss 0.8321\n",
      "step 99000: train loss 0.8676, val loss 0.8109\n",
      "step 99200: train loss 0.8504, val loss 0.8310\n",
      "step 99400: train loss 0.8313, val loss 0.8258\n",
      "step 99600: train loss 0.8501, val loss 0.8403\n",
      "step 99800: train loss 0.8564, val loss 0.8295\n",
      "step 100000: train loss 0.8653, val loss 0.8258\n",
      "saving\n",
      "step 100200: train loss 0.8405, val loss 0.8273\n",
      "step 100400: train loss 0.8685, val loss 0.8330\n",
      "step 100600: train loss 0.8517, val loss 0.8199\n",
      "step 100800: train loss 0.8807, val loss 0.8241\n",
      "step 101000: train loss 0.8441, val loss 0.8228\n",
      "step 101200: train loss 0.8567, val loss 0.8123\n",
      "step 101400: train loss 0.8448, val loss 0.8280\n",
      "step 101600: train loss 0.8488, val loss 0.8281\n",
      "step 101800: train loss 0.8643, val loss 0.8418\n",
      "step 102000: train loss 0.8580, val loss 0.8063\n",
      "saving\n",
      "step 102200: train loss 0.8584, val loss 0.8184\n",
      "step 102400: train loss 0.8506, val loss 0.8311\n",
      "step 102600: train loss 0.8412, val loss 0.8190\n",
      "step 102800: train loss 0.8477, val loss 0.8193\n",
      "step 103000: train loss 0.8520, val loss 0.7951\n",
      "step 103200: train loss 0.8434, val loss 0.8074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 103400: train loss 0.8517, val loss 0.8337\n",
      "step 103600: train loss 0.8548, val loss 0.8173\n",
      "step 103800: train loss 0.8453, val loss 0.8131\n",
      "step 104000: train loss 0.8308, val loss 0.8121\n",
      "saving\n",
      "step 104200: train loss 0.8615, val loss 0.8101\n",
      "step 104400: train loss 0.8557, val loss 0.8292\n",
      "step 104600: train loss 0.8440, val loss 0.8082\n",
      "step 104800: train loss 0.8414, val loss 0.8100\n",
      "step 105000: train loss 0.8451, val loss 0.8094\n",
      "step 105200: train loss 0.8417, val loss 0.8139\n",
      "step 105400: train loss 0.8568, val loss 0.8060\n",
      "step 105600: train loss 0.8422, val loss 0.8118\n",
      "step 105800: train loss 0.8379, val loss 0.8094\n",
      "step 106000: train loss 0.8557, val loss 0.8220\n",
      "saving\n",
      "step 106200: train loss 0.8463, val loss 0.8131\n",
      "step 106400: train loss 0.8442, val loss 0.8015\n",
      "step 106600: train loss 0.8378, val loss 0.8087\n",
      "step 106800: train loss 0.8258, val loss 0.7965\n",
      "step 107000: train loss 0.8355, val loss 0.7970\n",
      "step 107200: train loss 0.8335, val loss 0.8163\n",
      "step 107400: train loss 0.8525, val loss 0.8207\n",
      "step 107600: train loss 0.8354, val loss 0.8029\n",
      "step 107800: train loss 0.8373, val loss 0.8224\n",
      "step 108000: train loss 0.8401, val loss 0.8055\n",
      "saving\n",
      "step 108200: train loss 0.8328, val loss 0.8019\n",
      "step 108400: train loss 0.8447, val loss 0.8154\n",
      "step 108600: train loss 0.8580, val loss 0.8041\n",
      "step 108800: train loss 0.8470, val loss 0.8177\n",
      "step 109000: train loss 0.8484, val loss 0.8015\n",
      "step 109200: train loss 0.8544, val loss 0.8104\n",
      "step 109400: train loss 0.8389, val loss 0.7971\n",
      "step 109600: train loss 0.8247, val loss 0.8128\n",
      "step 109800: train loss 0.8377, val loss 0.8195\n",
      "step 110000: train loss 0.8323, val loss 0.7821\n",
      "saving\n",
      "step 110200: train loss 0.8306, val loss 0.8095\n",
      "step 110400: train loss 0.8294, val loss 0.7828\n",
      "step 110600: train loss 0.8416, val loss 0.7986\n",
      "step 110800: train loss 0.8287, val loss 0.8018\n",
      "step 111000: train loss 0.8282, val loss 0.8189\n",
      "step 111200: train loss 0.8457, val loss 0.8085\n",
      "step 111400: train loss 0.8086, val loss 0.7834\n",
      "step 111600: train loss 0.8261, val loss 0.7908\n",
      "step 111800: train loss 0.8455, val loss 0.7897\n",
      "step 112000: train loss 0.8429, val loss 0.7989\n",
      "saving\n",
      "step 112200: train loss 0.8317, val loss 0.7836\n",
      "step 112400: train loss 0.8390, val loss 0.8069\n",
      "step 112600: train loss 0.8364, val loss 0.7993\n",
      "step 112800: train loss 0.8395, val loss 0.8036\n",
      "step 113000: train loss 0.8235, val loss 0.7893\n",
      "step 113200: train loss 0.8356, val loss 0.8168\n",
      "step 113400: train loss 0.8226, val loss 0.7915\n",
      "step 113600: train loss 0.8356, val loss 0.7947\n",
      "step 113800: train loss 0.8217, val loss 0.7833\n",
      "step 114000: train loss 0.8359, val loss 0.7953\n",
      "saving\n",
      "step 114200: train loss 0.8356, val loss 0.7933\n",
      "step 114400: train loss 0.8332, val loss 0.7882\n",
      "step 114600: train loss 0.8346, val loss 0.7982\n",
      "step 114800: train loss 0.8256, val loss 0.8024\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10808\\3773312723.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iter in range(10000000 + 1):\n",
    "# while True:\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    if iter % (eval_interval * 10) == 0:\n",
    "        print(\"saving\")\n",
    "        torch.save(m, model_filepath + \".pt\")\n",
    "        torch.save(m.state_dict(), model_filepath + \"_state.pt\")\n",
    "        \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd112f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['START', 'Pawn:d2:TwoPush', 'Pawn:e7:Push', 'Pawn:e2:TwoPush', 'Pawn:d7:TwoPush', 'Pawn:e4:Push', 'Pawn:c7:Push', 'Pawn:f2:TwoPush', 'Pawn:f7:TwoPush', 'Pawn:c2:Push', 'Pawn:b7:Push', 'N2:-1:2', 'N2:-2:-1', 'BD:e3', 'Pawn:g7:Push', 'N1:2:1', 'BD:g7', 'Pawn:h2:Push', 'BL:a6', 'BL:d3', 'K:2:0', 'Q:c2', 'BL:d3', 'Q:d3', 'N1:2:-1', 'K:-2:0', 'Pawn:c6:Push', 'Pawn:g2:TwoPush', 'R1:2:0', 'K:-1:0', 'Pawn:b6:Push', 'Q:e2', 'Pawn:c5:Push', 'Pawn:g4:Push', 'Pawn:b5:Push', 'Pawn:c3:CaptureLeft', 'N2:-2:-1', 'Pawn:h3:Push', 'N2:-1:-2', 'K:-1:0', 'Pawn:c4:Push', 'Pawn:b2:CaptureRight', 'N2:-1:-2', 'K:1:1', 'N2:2:-1', 'R2:-3:0', 'Pawn:h7:TwoPush', 'Pawn:g5:CaptureRight', 'K:1:-1', 'Pawn:h6:CaptureLeft', 'K:-1:0', 'Pawn:h4:Push', 'K:-1:0', 'N2:1:2', 'K:-1:1', 'N2:-2:-1', 'Q:a5', 'N2:-1:2', 'K:-1:0', 'N2:2:-1', 'Q:b5', 'Q:b5', 'R1:0:-3', 'N2:-1:2', 'K:1:-1', 'R2:1:0', 'K:-1:1', 'Pawn:h5:Push', 'K:1:-1', 'Pawn:h6:Push', 'R2:2:0', 'K:-1:1', 'K:-1:1', 'K:0:1', 'K:-1:-1', 'K:0:1', 'K:1:1', 'K:0:1', 'K:1:-1', 'K:0:1', 'K:1:1', 'K:1:0', 'K:1:-1', 'K:-1:0', 'K:-1:0', 'K:0:-1', 'K:-1:1', 'K:0:-1', 'K:0:-1', 'K:0:-1', 'K:1:0', 'K:0:-1', 'K:-1:0', 'K:0:1', 'K:1:0', 'K:0:-1', 'K:-1:0', 'K:1:1', 'K:1:1', 'K:-1:0', 'K:1:-1']\n",
      "1. d4 e6 2. e4 d5 3. e5 c6 4. f4 f5 5. c3 b6 6. Nf3 Ne7 7. Be3 g6 8. Nbd2 Bg7 9. h3 Ba6 10. Bd3 O-O 11. Qc2 Bxd3 12. Qxd3 Nd7 13. O-O-O c5 14. g4 Rc8 15. Kb1 b5 16. Qe2 c4 17. g5 b4 18. cxb4 Nc6 19. h4 Nxb4 20. Ka1 c3 21. bxc3 Nxa2 22. Kb2 Nc1 23. Rhe1 h5 24. gxh6 Kh7 25. hxg7 Kxg7 26. h5 Kf7 27. Ng5+ Ke8 28. Nge4 Qa5 29. Nd6+ Kd8 30. Nxf5 Qb5+ 31. Qxb5 Rc5 32. Ne7 Kxe7 33. Rf1 Kd8 34. h6 Ke7 35. h7 Rh8 36. Ka3 Kd8 37. Ka4 Kc7 38. Ka5 Kd8 39. Ka6 Ke7 40. Kxa7 Kf8 41. Kb7 Kg7 42. Ka7 Kf7 43. Ka6 Ke8 44. Ka5 Ke7 45. Ka4 Kf7 46. Ka3 Ke7 47. Ka4 Kf7 48. Ka3 Ke7 49. Kb4 Kf8 50. Ka4 Kg7\n"
     ]
    }
   ],
   "source": [
    "game = chess_game_tracker.Game()\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "context[0][0] = 1\n",
    "print(decode(generate(m, idx = context, max_new_tokens=100, game=game)[0].tolist()))\n",
    "print (game.get_game_pgn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bd32776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m, model_filepath + \".pt\")\n",
    "torch.save(m.state_dict(), model_filepath + \"_state.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142dc488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
