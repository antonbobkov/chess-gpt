{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e157b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess_game_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df2bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "tokens_filepath = \"tokens.txt\"\n",
    "model_filepath = \"v1_model\"\n",
    "\n",
    "# hyperparameters\n",
    "block_size = 64 # what is the maximum context length for predictions?\n",
    "eval_iters = 100\n",
    "n_embd = 400\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "746f47de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n"
     ]
    }
   ],
   "source": [
    "with open(tokens_filepath, \"r\") as f:\n",
    "    tokens = f.read().splitlines()\n",
    "    \n",
    "tokens = [\"ZERO\", \"START\"] + tokens\n",
    "\n",
    "token_to_int = {}\n",
    "int_to_token = {}\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    token_to_int[token] = i\n",
    "    int_to_token[i] = token\n",
    "    \n",
    "def encode(lst):\n",
    "    return [token_to_int[token] for token in lst]\n",
    "def decode(lst):\n",
    "    return [int_to_token[i] for i in lst]\n",
    "\n",
    "vocab_size = len(tokens)\n",
    "print (vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "392af055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01973766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, game):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop idx to the last block_size tokens\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        # get the predictions\n",
    "        logits, loss = model(idx_cond)\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "        legal_moves = encode(game.encoded_legal_moves())\n",
    "#         print (legal_moves)\n",
    "        if len(legal_moves) == 0:\n",
    "            return idx\n",
    "\n",
    "        for i in range(vocab_size):\n",
    "            if i not in legal_moves:\n",
    "                logits[0, i] = -100000\n",
    "        \n",
    "        # apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "        \n",
    "#         print (idx_next,  idx_next.tolist()[0], \"hi\")\n",
    "        assert idx_next.tolist()[0][0] in legal_moves\n",
    "        \n",
    "        game.make_move(decode(idx_next.tolist()[0])[0])\n",
    "        \n",
    "        # append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "551e15a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.01076 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65803c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_filepath + \"_state.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba185f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['START', 'Pawn:e2:TwoPush', 'Pawn:g7:Push', 'Pawn:d2:TwoPush', 'N1:-1:-2', 'Pawn:c2:Push', 'Pawn:e7:Push', 'BD:g5', 'BD:e7', 'BD:f6', 'BD:f6', 'BL:c4', 'Pawn:c7:Push', 'N2:-2:1', 'Pawn:d7:TwoPush', 'Pawn:e4:CaptureLeft', 'Pawn:c6:CaptureRight', 'BL:b3', 'BL:d7', 'K:2:0', 'N2:-2:-1', 'N2:2:1', 'BD:g5', 'Pawn:h2:Push', 'BL:c6', 'Pawn:a2:Push', 'Pawn:b7:TwoPush', 'N1:2:1', 'K:2:0', 'Q:f3', 'N1:1:-2', 'Q:g4', 'BD:h6', 'N1:1:2', 'Pawn:f7:TwoPush', 'Q:d1', 'Pawn:f5:CaptureLeft', 'N2:-1:2', 'Q:e8', 'N2:1:-2', 'R1:2:0', 'N2:-1:2', 'Q:d7', 'Pawn:g2:Push', 'Pawn:e6:Push', 'N2:2:1', 'K:0:-1', 'N2:-2:1', 'R1:2:0', 'K:1:1', 'Pawn:h7:TwoPush', 'N2:-2:1', 'K:1:-1', 'Q:d3', 'Q:e6', 'Pawn:g3:Push', 'Q:f7', 'Q:g3', 'Pawn:a7:TwoPush', 'R1:1:0', 'N1:2:-1', 'Q:f3', 'R2:1:0', 'R2:2:0', 'Pawn:a5:Push', 'R2:-4:0', 'Pawn:a4:CaptureRight', 'R2:0:1', 'N1:-2:1', 'Q:d1', 'N2:1:-2', 'Pawn:h3:Push', 'R2:1:0', 'Q:f3', 'Pawn:h5:CaptureLeft', 'Q:e2', 'N2:-1:2', 'K:-1:0', 'Pawn:g4:Push', 'K:0:1', 'N2:1:-2', 'K:0:-1', 'N2:-1:2', 'Q:f3', 'R2:-1:0', 'Q:e3', 'K:-1:1', 'Q:f3', 'R2:-1:0', 'Q:g3', 'Q:f6', 'R1:3:0', 'R2:1:0', 'R1:0:2', 'Q:h4', 'Q:h3', 'Q:h7', 'K:0:1', 'Q:h6', 'R2:1:0', 'Q:f4']\n",
      "1. e4 g6 2. d4 Na6 3. c3 e6 4. Bg5 Be7 5. Bf6 Bxf6 6. Bc4 c6 7. Ne2 d5 8. exd5 cxd5 9. Bb3 Bd7 10. O-O Ne7 11. Ng3 Bg5 12. h3 Bc6 13. a3 b5 14. Nd2 O-O 15. Qf3 Nb4 16. Qg4 Bh6 17. Nde4 f5 18. Qd1 fxe4 19. Nf5 Qe8 20. Ng3 Rc8 21. Nf5 Qd7 22. g3 e5 23. Nxh6+ Kg7 24. Nf7 Rce8 25. Kh2 h5 26. Nd8 Kh6 27. Qd3 Qe6 28. g4 Qf7 29. Qg3 a5 30. Rab1 Nd3 31. Qf3 Rg8 32. Rh1 a4 33. Rhd1 axb3 34. Rd2 Nb4 35. Qd1 Nf5 36. h4 Rh8 37. Qf3 hxg4 38. Qe2 Ne7 39. Kg2 g3 40. Kxg3 Nf5+ 41. Kg2 Ne7 42. Qf3 Rhg8 43. Qe3+ Kg7 44. Qf3 Rgf8 45. Qg3 Qf6 46. Re1 Rg8 47. Re3 Qxh4 48. Qh3 Qh7 49. Kg3 Qh6 50. Rde2 Qf4+\n"
     ]
    }
   ],
   "source": [
    "game = chess_game_tracker.Game()\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "context[0][0] = 1\n",
    "print(decode(generate(model, idx = context, max_new_tokens=100, game=game)[0].tolist()))\n",
    "print (game.get_game_pgn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8ad715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncodeMoveSequence(move_list):\n",
    "    game = chess_game_tracker.Game()\n",
    "    for dec_move in move_list:\n",
    "        game.make_dec_move(dec_move)\n",
    "    return game, encode(['START'] + game.enc_moves)\n",
    "\n",
    "def GetWeightedMovesFromModel(mdl, move_list):\n",
    "    game, enc_moves = EncodeMoveSequence(move_list)\n",
    "    #print (enc_moves, decode(enc_moves))\n",
    "\n",
    "    t = torch.tensor(enc_moves, dtype=torch.long)\n",
    "    t = t[-block_size:]\n",
    "    t = t[None, :]\n",
    "    #print (t, t.shape)\n",
    "\n",
    "    logits, _ = mdl(t)\n",
    "    logits = logits[:, -1, :]\n",
    "    logits = logits[0]\n",
    "\n",
    "    # print (logits.shape)\n",
    "\n",
    "    legal = game.encoded_legal_moves()\n",
    "    moves_and_weights = list(enumerate(logits.tolist()))\n",
    "    moves_and_weights = sorted(moves_and_weights, key=lambda x: -x[1])\n",
    "    \n",
    "    final_list = []\n",
    "\n",
    "    for i, val in moves_and_weights:\n",
    "        if int_to_token[i] in legal:\n",
    "            final_list.append((int_to_token[i], game.decode_possible_move(int_to_token[i]), val))\n",
    "        else:\n",
    "            final_list.append((int_to_token[i], None, val))\n",
    "    return final_list\n",
    "\n",
    "def GetTopLegalMove(moves_and_weights):\n",
    "    for _, mv, _ in moves_and_weights:\n",
    "        if mv is not None:\n",
    "            return mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8102a56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g1f3\n"
     ]
    }
   ],
   "source": [
    "moves_and_weights = GetWeightedMovesFromModel(model, \n",
    "    [\n",
    "        chess.Move.from_uci('e2e4'),\n",
    "        chess.Move.from_uci('e7e5'),\n",
    "    ])\n",
    "# for tpl in moves_and_weights:\n",
    "#     print (tpl)\n",
    "print (GetTopLegalMove(moves_and_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0cd2008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. e4 e5 2. Nf3 Nc6 3. Bc4 Bc5 4. c3 Nf6 5. d4 exd4 6. cxd4 Bb4+ 7. Nc3 Nxe4 8. O-O Nxc3 9. bxc3 Be7 10. d5 Na5 11. Bd3 O-O 12. Qa4 b6 13. Bd2 Bf6 14. Rae1 d6 15. c4 Nb7 16. Re2 Nc5 17. Qc2 Nxd3 18. Qxd3 Bg4 19. Rfe1 Qd7 20. Bc3 Bxc3 21. Qxc3 Bxf3 22. Qxf3 Rae8 23. h3 Rxe2 24. Qxe2 g6 25. Qe4 Kg7 26. a4 a5 27. Qd4+ Kg8 28. Qd1 Re8 29. Rf1 Re4 30. Qd3 Qe7 31. Qc3 Qe8 32. Qe3 Rxe3 33. Kh2 Re6 34. f4 Qe7 35. f5 g5 36. Re1 Qd8 37. Rc1 Re3 38. Kg1 c6 39. c5 dxc5 40. Rc3 Rd3 41. Rxc5 Rc3 42. Kf2 Rc2+ 43. Ke3 Rc3+ 44. Ke4 Rc2 45. Rb5 Rc3 46. Ke5 Rc4 47. g4 h5 48. gxh5 Qd6+ 49. Kxd6 Kg7 50. h4 Kg8\n"
     ]
    }
   ],
   "source": [
    "moves = []\n",
    "\n",
    "for i in range(100):\n",
    "    moves_and_weights = GetWeightedMovesFromModel(model, moves)\n",
    "    move = GetTopLegalMove(moves_and_weights)\n",
    "    # print (move)\n",
    "    moves.append(move)\n",
    "    \n",
    "print (chess.Board().variation_san(moves))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c810fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
